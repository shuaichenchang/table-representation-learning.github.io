
<br />


<div class="row">
    <div class="col-xs-12">
        <br>
        <b style="color: red">TRL 2023 is a wrap! Interested in updates for future events, news, etc? Sign up for the <a href="https://forms.gle/s3qyD5VTkaYbv1W58" target="blank">mailinglist here</a>!<br><br></b>
    </div>

    <div class="col-xs-12">
        <p>
            <b>Tables are a promising modality for representation learning with too much application potential to ignore.</b>
            However, tables have long been overlooked despite their dominant presence in the data landscape, e.g. data management and analysis pipelines.
            The majority of datasets in Google Dataset Search, for example, resembles typical tabular file formats like CSVs.
            Similarly, the top-3 most-used database management systems are all intended for relational data (RDBMS).
            Representation learning over tables, possibly combined with other modalities such as text or SQL, has shown impressive performance for tasks like semantic parsing,
            question answering, table understanding, and data preparation. More recently, the pre-training paradigm was shown to be effective for tabular ML as well,
            while researchers also started exploring the impressive capabilities of LLMs for table encoding and data manipulation.
        </p>
        <p>
            The Table Representation Learning (TRL) workshop is the first in this emerging research area and
            concentrates on three main goals:
        <ul>
            <li> (1) Motivate tables as a primary modality for representation and generative learning and advance the area further. </li>
            <li> (2) Showcase impactful applications of pretrained table models and discussing future opportunities. </li>
            <li> (3) Foster discussion and collaboration across the ML, NLP and DB communities. </li>
        </ul>
        </p>
    </div>

    <div >
        <div class="col-xs-12">
            <!-- <div>
                Frequently Asked Questions: <a href="https://groups.google.com/g/table-representation-learning-workshop"
                    target="blank">FAQ</a>
            </div> -->
            <div>
                When: Friday 15 December 2023, 8:30am - 5:30pm (local time).<br>
                Where: Room 235-236, New Orleans Ernest N. Morial Convention Center.<br>
            </div>
            <div>
                Submit:
                <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL" target="blank">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL</a>
            </div>
            <div>
                Public questions: <a class="u-email"
                    href="https://groups.google.com/g/table-representation-learning-workshop"target="blank">table-representation-learning-workshop@googlegroups.com</a>
            </div>
            <div>
                Private questions: <a class="u-email" href="mailto:madelon@berkeley.edu"  target="blank">madelon@berkeley.edu</a>
            </div>
            <div>
                <b>Follow on Twitter</b>:
                <i aria-hidden="true" class="fa fa-twitter"></i>
                <a class="menulink" href="https://twitter.com/TrlWorkshop" target="_blank">@TrlWorkshop</a>
            </div>
            <div>
                Sponsored by: <a href="https://www.approximatelabs.com/" target="blank"><img src="/assets/approximate-logo.svg" width="250"></a>
            </div>

        </div>
    </div>
</div>


    <br>

    <hr />

        <!-- Program -->
        <div class="row" id="program">
            <div class="col-xs-12">
                <h2>Program</h2>

                TRL is again entirely in-person, and will this year feature 2 longer poster sessions and more (spotlight) talks on contributed work.<br>
                We also host a few exciting invited talks on established research in this emerging area, and a panel discussion.<br>

                <br>

                <h3>Invited Speakers</h3>
                <br>
                <div class="row">
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://wenhuchen.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/wc.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://wenhuchen.github.io/" target="blank">Wenhu Chen</a>
                            <h6>University of Waterloo,<br>Google DeepMind, Vector Institute</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://ml.informatik.uni-freiburg.de/profile/hutter/" target="blank">
                            <img class="people-pic" src="/assets/people/fh.png">
                        </a>
                        <div class="people-name">
                            <a href="https://ml.informatik.uni-freiburg.de/profile/hutter/" target="blank">Frank Hutter</a>
                            <h6>University of Freiburg</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://arorasimran.com/" target="blank">
                            <img class="people-pic" src="/assets/people/sa.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://arorasimran.com/" target="blank">Simran Arora</a>
                            <h6>Stanford University</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://itrummer.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/it.jpeg">
                        </a>
                        <div class="people-name">
                            <a href="https://itrummer.github.io/" target="blank">Immanuel Trummer</a>
                            <h6>Cornell University</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://taoyds.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/ty.jpeg">
                        </a>
                        <div class="people-name">
                            <a href="https://taoyds.github.io/" target="blank">Tao Yu</a>
                            <h6>University of Hong Kong</h6>
                        </div>
                        <br>
                        <br>
                    </div>
                </div>
                
                <div>
                    <h3>Panelists (TBC)</h3>
                    <br>

                    <div class="col-xs-6 col-lg-2">
                        <a href="https://www.eurecom.fr/~papotti/" target="blank">
                            <img class="people-pic" src="/assets/people/pp.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://www.eurecom.fr/~papotti/" target="blank">Paolo Papotti (moderator)</a>
                            <h6>Eurecom</h6>
                        </div>
                        <br>
                        <br>
                    </div>

                    <div class="col-xs-6 col-lg-2">
                        <a href="https://amueller.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/am.jpeg">
                        </a>
                        <div class="people-name">
                            <a href="https://amueller.github.io/" target="blank">Andreas Mueller</a>
                            <h6>Microsoft</h6>
                        </div>
                        <br>
                        <br>
                    </div>

                    <div class="col-xs-6 col-lg-2">
                        <a href="https://wenhuchen.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/wc.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://wenhuchen.github.io/" target="blank">Wenhu Chen</a>
                            <h6>University of Waterloo</h6>
                        </div>
                        <br>
                        <br>
                    </div>

                    <div class="col-xs-6 col-lg-2">
                        <a href="https://www.vcherepanova.com/" target="blank">
                            <img class="people-pic" src="/assets/people/vc.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://www.vcherepanova.com/" target="blank">Valeriia Cherepanova</a>
                            <h6>Amazon</h6>
                        <br>
                        </div>
                        <br>
                    </div>

                    <div class="col-xs-6 col-lg-2">
                        <!-- <a href="https://goldblum.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/mg.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://goldblum.github.io/" target="blank">Micah Goldblum</a>
                            <h6>New York University</h6>
                            <br>
                        </div>
                        <br>
                        <br> -->
                        <br>
                        <br>
                        <br>
                        <br>
                        <br>
                        <br>
                        <br>
                        <br>
                        <br>
                        <br>
                    </div>
                </div>

                <div class="row" id="schedule">
                    <br>
                    <div class="col-md-12 col-xs-12">
                        <br>
                        <h3>Schedule</h3>
                        <br>
                    </div>
                    <div class="col-md-8 col-xs-12">
                        <select id="timezone-select" class="form-control"></select>
                    </div>
                </div>
                <div class="row">
                    <div class="col-xs-12">
                        <table class="table table-striped" id="schedule-table">
                            <tbody>
                                <tr>
                                    <th scope="row" data-time="06:30">06:30 AM</th>
                                    <td>Opening Notes</td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="06:45">06:45 AM</th>
                                    <td>
                                        Invited Talk: Simran Arora<br />
                                        <span style="font-style:italic">Co-Designing LLMs and LLM-Powered Data Management Tools</span>
                                        <br/>
                                        <a data-toggle="collapse" href="#schedule-talk1" aria-cexpanded="false"
                                            aria-controls="schedule-talk1">[Abstract]</a>
                                        <div class="collapse" id="schedule-talk1">
                                            Abstract: Large Language Models (LLMs) are now widely used for data management. We recently proposed Evaporate [ICLR Spotlight 2023, VLDB 2024], a system that uses LLMs to help users efficiently query semi-structured documents. We also showed how off-the-shelf LLMs perform data-wrangling tasks with state-of-the-art quality and no specialized training [VLDB 2023]. This talk discusses some of my lessons from working on these early LLM-for-data-management projects and subsequent research to improve the reach of these systems — in particular, there is a ways to go for extending LLMs to datatypes such as private, semi-structured, and long-sequence data. Towards extending our capabilities on these datatypes, I’ll discuss MQAR and Monarch Mixer [NeurIPS Oral 2023], new LM architectures that can match the quality of attention-based LMs, while remaining asymptotically more efficient at training and inference time. We’ll finally discuss how these fundamental breakthroughs can power next-generation data management tools.
                                        </div>
                                        <a data-toggle="collapse" href="#speaker-bio-talk1" aria-cexpanded="false"
                                        aria-controls="speaker-bio-talk1">[Speaker Bio]</a>
                                        <div class="collapse" id="speaker-bio-talk1">
                                            Bio: Simran Arora is a PhD student at Stanford University in machine learning systems, advised by Chris Ré. She develops tools that help users apply foundation models to challenging datatypes such as private, semi-structured, and long-sequence data. To unlock these capabilities, she leverages a detailed understanding of the data and inductive biases used to train foundation models. Her work has received Oral (top 5%) and Spotlight (top 25%) awards at NeurIPS and ICLR. Simran also recently created and taught a new [Systems for Machine Learning](https://cs229s.stanford.edu/fall2023/) full-unit course at Stanford. She is grateful for the support of the SGF Sequoia Fellowship.
                                        </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="07:15">07:15 AM</th>
                                    <td>Spotlight Talks: Session 1 (Data Wrangling and Table QA)</td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="07:15">07:15 AM</span> ~
                                    <span scope="row" data-time="07:22">07:22 AM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=gs6yfSvwue" target="_blank">High-Performance Transformers for Table Structure Recognition Need Early Convolutions </a>
                                    <br/>
                                    <span class="paper-authors">Anthony Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari (Raji) Balasubramaniyan, Duen Horng Chau
                                    </span>
                                    <a data-toggle="collapse" href="#spotlight-paper-1" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-1">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-1">
                                        Abstract: Table structure recognition (TSR) aims to convert tabular images into a machine-readable format, where a visual encoder extracts image features and a textual decoder generates table-representing tokens. Existing approaches use classic convolutional neural network (CNN) backbones for the visual encoder and transformers for the textual decoder. However, this hybrid CNN-Transformer architecture introduces a complex visual encoder that accounts for nearly half of the total model parameters, markedly reduces both training and inference speed, and hinders the potential for self-supervised learning in TSR. In this work, we design a lightweight visual encoder for TSR without sacrificing expressive power. We discover that a convolutional stem can match classic CNN backbone performance, with a much simpler model. The convolutional stem strikes an optimal balance between two crucial factors for high-performance TSR: a higher receptive field (RF) ratio and a longer sequence length. This allows it to "see" an appropriate portion of the table and "store" the complex table structure within sufficient context length for the subsequent transformer. We conducted reproducible ablation studies and open-sourced our code at https://anonymous.4open.science/r/NeurIPS23-TRL-2 to enhance transparency, inspire innovations, and facilitate fair comparisons in our domain as tables are a promising modality for representation learning.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="07:23">07:23 AM</span> ~
                                    <span scope="row" data-time="07:30">07:30 AM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=6Kb3pE9nWQ" target="_blank">Pool-Search-Demonstrate: Improving Data-wrangling LLMs via better in-context examples </a>
                                    <br>
                                    <span class="paper-authors">Joon Suk Huh, Changho Shin, Elina Choi
                                    </span>
                                    <a data-toggle="collapse" href="#spotlight-paper-2" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-2">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-2">
                                        Abstract: Data-wrangling is a process that transforms raw data for further analysis and for use in downstream tasks. Recently, it has been shown that foundation models can be successfully used for data-wrangling tasks (Narayan et. al., 2022). An important aspect of data wrangling with LMs is to properly construct prompts for the given task. Within these prompts, a crucial component is the choice of in-context examples. In the previous study of Narayan et. al., demonstration examples are chosen manually by the authors, which may not be scalable to new datasets. In this work, we propose a simple demonstration strategy that individualizes demonstration examples for each input by selecting them from a pool based on their distance in the embedding space. Additionally, we propose a postprocessing method that exploits the embedding of labels under a closed-world assumption. Empirically, our embedding-based example retrieval and postprocessing improve foundation models' performance by up to 84\% over randomly selected examples and 49\% over manually selected examples in the demonstration. Ablation tests reveal the effect of class embeddings, and various factors in demonstration such as quantity, quality, and diversity.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="07:31">07:31 AM</span> ~
                                    <span scope="row" data-time="07:38">07:38 AM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=4MkkNsAEmO" target="_blank">TabPFGen – Tabular Data Generation with TabPFN</a>
                                    <br>
                                    <span class="paper-authors">Jeremy (Junwei) Ma, Apoorv Dankar, George Stein, Guangwei Yu, Anthony Caterini
                                    </span>
                                    <a data-toggle="collapse" href="#spotlight-paper-3" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-3">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-3">
                                        Abstract: Advances in deep generative modelling have not translated well to tabular data. We argue that this is caused by a mismatch in structure between popular generative models and discriminative models of tabular data. We thus devise a technique to turn TabPFN -- a highly performant transformer initially designed for in-context discriminative tabular tasks -- into an energy-based generative model, which we dub TabPFGen. This novel framework leverages the pre-trained TabPFN as part of the energy function and does not require any additional training or hyperparameter tuning, thus inheriting TabPFN's in-context learning capability. We can sample from TabPFGen analogously to other energy-based models. We demonstrate strong results on standard generative modelling tasks, including data augmentation, class-balancing, and imputation, unlocking a new frontier of tabular data generation.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="07:38">07:38 AM</span> ~
                                    <span scope="row" data-time="07:45">07:45 AM</span>    
                                    <a class="paper-title" href="https://openreview.net/forum?id=FflKTuIRTD" target="_blank">Data Ambiguity Strikes Back: How Documentation Improves GPT's Text-to-SQL</a>
                                    <br>
                                    <span class="paper-authors">Zachary Huang, Pavan Kalyan Damalapati, Eugene Wu
                                    </span>
                                    <a data-toggle="collapse" href="#spotlight-paper-4" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-4">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-4">
                                        Abstract: Text-to-SQL allows experts to use databases without in-depth knowledge of them. However, real-world tasks have both query and data ambiguities. Most works on Text-to-SQL focused on query ambiguities and designed chat interfaces for experts to provide clarifications.In contrast, the data management community has long studied data ambiguities, but mainly addresses error detection and correction, rather than documenting them for disambiguation in data tasks. This work delves into these data ambiguities in real-world datasets. We have identified prevalent data ambiguities of value consistency, data coverage, and data granularity that affect tasks. We examine how documentation, originally made to help humans to disambiguate data, can help GPT-4 with Text-to-SQL tasks. By offering documentation on these, we found GPT-4's performance improved by 28.9%.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="07:46">07:46 AM</span> ~
                                    <span scope="row" data-time="07:53">07:53 AM</span>        
                                    <a class="paper-title" href="https://openreview.net/forum?id=zxaoBcdACd" target="_blank">MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering</a>
                                    <br>
                                    <span class="paper-authors">Vaishali Pal, Andrew Yates, Evangelos Kanoulas, Maarten Rijke
                                    </span>
                                    <a data-toggle="collapse" href="#spotlight-paper-5" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-5">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-5">
                                        Abstract: Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels of granularity of the table structure. MultiTabQA outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.
                                    </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="08:00">08:00 AM</th>
                                    <td>Coffee Break / Poster Setup</td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="08:20">08:20 AM</th>
                                    <td>Poster: Session 1
                                    <a data-toggle="collapse" href="#poster-1" aria-cexpanded="false"
                                    aria-controls="poster-1">[Details]</a>
                                    <div class="collapse" id="poster-1">
                                    <li>MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering. Vaishali Pal, Andrew Yates, Evangelos Kanoulas, Maarten Rijke.</li>

                                    <li>Generating Data Augmentation Queries Using Large Language Models.Christopher Buss, Jasmin Mousavi, Mikhail Tokarev, Arash Termehchy, David Maier, Stefan Lee.</li>

                                    <li>ReConTab: Regularized Contrastive Representation Learning for Tabular Data. Suiyao Chen, Jing Wu, Naira Hovakimyan, Handong Yao.</li>

                                    <li>Explaining Explainers: Necessity and Sufficiency in Tabular Data. Prithwijit Chowdhury, Mohit Prabhushankar, Ghassan AlRegib.</li>

                                    <li>Beyond Individual Input for Deep Anomaly Detection on Tabular Data. Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Liên DOAN.</li>

                                    <li>InterpreTabNet: Enhancing Interpretability of Tabular Data Using Deep Generative Models and Large Language Models. Jacob Yoke Hong Si, Michael Cooper, Wendy Yusi Cheng, Rahul Krishnan.</li>

                                    <li>On Incorporating new Variables during Evaluation. Harsimran Bhasin, Soumyadeep Ghosh.</li>

                                    <li>GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data. Andrei Margeloiu, Nikola Simidjievski, Pietro Lio, Mateja Jamnik.</li>

                                    <li>High-Performance Transformers for Table Structure Recognition Need Early Convolutions. Anthony Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari (Raji) Balasubramaniyan, Duen Horng Chau.</li>

                                    <li>Unnormalized Density Estimation with Root Sobolev Norm Regularization. Mark Kozdoba, Binyamin Perets, Shie Mannor.</li>

                                    <li>Tree-Regularized Tabular Embeddings. Xuan Li, Yun Wang, Bo Li.</li>

                                    <li>A Deep Learning Blueprint for Relational Databases. Lukáš Zahradník, Jan Neumann, Gustav Šír.</li>

                                    <li>Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks. Benjamin Feuer, Niv Cohen, Chinmay Hegde.</li>

                                    <li>NeuroDB: Efficient, Privacy-Preserving and Robust Query Answering with Neural Networks. Sepanta Zeighami, Cyrus Shahabi.</li>

                                    <li>A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning. Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping, C. Bruss, Andrew Wilson, Tom Goldstein, Micah Goldblum.</li>

                                    <li>Benchmarking Tabular Representation Models in Transfer Learning Settings. Qixuan Jin, Talip Ucar.</li>

                                    <li>Data Ambiguity Strikes Back: How Documentation Improves GPT's Text-to-SQL. Zezhou Huang, Pavan Kalyan Damalapati, Eugene Wu.</li>

                                    <li>Pool-Search-Demonstrate: Improving Data-wrangling LLMs via better in-context examples. Joon Suk Huh, Changho Shin, Elina Choi.</li>

                                    <li>TabPFGen – Tabular Data Generation with TabPFN. Junwei Ma, Apoorv Dankar, George Stein, Guangwei Yu, Anthony Caterini.</li>

                                    <li>Multitask-Guided Self-Supervised Tabular Learning for Patient-Specific Survival Prediction. You Wu, Omid Bazgir, Yongju Lee, Tommaso Biancalani, James Lu, Ehsan Hajiramezanali.</li>

                                </td>
                                </div>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="09:00">09:00 AM</th>
                                    <td>
                                        Invited Talk: Frank Hutter<br />
                                        <span style="font-style:italic">Advances in In-Context Learning for Tabular Datasets</span>
                                        <br/>
                                        <a data-toggle="collapse" href="#schedule-talk2" aria-cexpanded="false"
                                            aria-controls="schedule-talk2">[Abstract]</a>
                                        <div class="collapse" id="schedule-talk2">
                                            Abstract: A year ago, we introduced TabPFN, the first in-context learning method for tabular data. In this talk, I will discuss what happened since. I will start by briefly discussing CAAFE, a system that uses LLMs for automated feature engineering on tabular data and makes effective use of TabPFN's speed. Then, I will situate prior-fitted PFNs in the in-context learning literature, review various applications of PFN, explain TabPFN in some more detail and then discuss our ongoing work on removing TabPFN's remaining limitations.
                                        </div>
                                        <a data-toggle="collapse" href="#speaker-bio-talk2" aria-cexpanded="false"
                                        aria-controls="speaker-bio-talk2">[Speaker Bio]</a>
                                        <div class="collapse" id="speaker-bio-talk2">
                                            Bio: Frank Hutter is a Full Professor for Machine Learning at the University of Freiburg (Germany). He holds a PhD from the University of British Columbia (UBC, 2009) and a Diplom (eq. MSc) from TU Darmstadt (2004). He received the 2010 CAIAC doctoral dissertation award for the best thesis in AI in Canada, and with his coauthors, several best paper awards and prizes in international competitions on machine learning, SAT solving, and AI planning. He is a Fellow of EurAI and ELLIS, the director of the ELLIS unit Freiburg and the recipient of 3 ERC grants. Frank is best known for his research on automated machine learning (AutoML), including neural architecture search and efficient hyperparameter optimization. He co-authored the first book on AutoML and the prominent AutoML tools Auto-WEKA, Auto-sklearn and Auto-PyTorch, won the first two AutoML challenges with his team, co-organized the ICML workshop series on AutoML every year 2014-2021, has been the general chair of the inaugural AutoML conference 2022 and is general chair again in 2023.
                                        </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="09:30">09:30 AM</th>
                                    <td>Spotlight Talks: Session 2 (Tabular ML)</td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="09:30">09:30 AM</span> ~
                                    <span scope="row" data-time="09:37">09:37 AM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=dkeZPuFmIz" target="_blank">Self-supervised Representation Learning from Random Data Projectors</a>
                                    <br/>
                                    <span class="paper-authors">Yi Sui, Tongzi Wu, Jesse Cresswell, Ga Wu, George Stein, Xiao Shi Huang, Xiaochen Zhang, Maksims Volkovs</span>
                                    <a data-toggle="collapse" href="#spotlight-paper-2-1" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-2-1">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-2-1">
                                        Abstract: Self-supervised representation learning SSRL has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities such as tabular and time-series data. This paper presents an SSRL approach that can be applied to these data modalities because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on real-world applications with tabular and time-series data. We show that it outperforms multiple state-of-the-art SSRL baselines and is competitive with methods built on domain-specific knowledge. Due to its wide applicability and strong empirical results, we argue that learning from randomness is a fruitful research direction worthy of attention and further study.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="09:38">09:38 AM</span> ~
                                    <span scope="row" data-time="09:45">09:45 AM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=jSebr3OJA2" target="_blank">GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data</a>
                                    <br>
                                    <span class="paper-authors">Andrei Margeloiu, Nikola Simidjievski, Pietro Lió, Mateja Jamnik</span>
                                    <a data-toggle="collapse" href="#spotlight-paper-2-2" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-2-2">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-2-2">
                                        Abstract: Neural network models often struggle with high-dimensional but small sample-size tabular datasets. One reason is that current weight initialisation methods assume independence between weights, which can be problematic when there are insufficient samples to estimate the model's parameters accurately. In such small data scenarios, leveraging additional structures can improve the model's performance and training stability. To address this, we propose GCondNet, a general approach to enhance neural networks by leveraging implicit structures present in tabular data. We create a graph between samples for each data dimension, and utilise Graph Neural Networks (GNNs) for extracting this implicit structure, and for conditioning the parameters of the first layer of an underlying predictor network. By creating many small graphs, GCondNet exploits the data's high-dimensionality, and thus improves the performance of an underlying predictor network. We demonstrate the effectiveness of our method on 9 real-world datasets, where GCondNet outperforms 15 standard and state-of-the-art methods. The results show that GCondNet is a versatile framework for injecting graph-regularisation into various types of neural networks, including MLPs and tabular Transformers.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="09:46">09:46 AM</span> ~
                                    <span scope="row" data-time="09:53">09:53 AM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=VRBhaU8IDz" target="_blank">HyperFast: Instant Classification for Tabular Data</a>
                                    <br>
                                    <span class="paper-authors">David Bonet, Daniel Mas Montserrat, Xavier Giró-i-Nieto, Alexander Ioannidis</span>
                                    <a data-toggle="collapse" href="#spotlight-paper-2-3" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-2-3">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-2-3">
                                        Abstract: Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming. Meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass. HyperFast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model. We report extensive experiments with OpenML and genomic data, comparing HyperFast to competing tabular data neural networks, traditional ML methods, AutoML systems, and boosting machines. HyperFast shows highly competitive results, while being significantly faster. Additionally, our approach demonstrates robust adaptability across a variety of classification tasks with little to no fine-tuning, positioning HyperFast as a strong solution for numerous applications and rapid model deployment. HyperFast introduces a promising paradigm for fast classification, with the potential to substantially decrease the computational burden of deep learning. Our code, which offers a scikit-learn-like interface, along with the trained HyperFast model, can be found at www.url-hidden-for-submission.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="09:54">09:54 AM</span> ~
                                    <span scope="row" data-time="10:01">10:01 AM</span>    
                                    <a class="paper-title" href="https://openreview.net/forum?id=UkP1BSm2tt" target="_blank">Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation</a>
                                    <br>
                                    <span class="paper-authors">Han-Jia Ye, Qile Zhou, De-Chuan Zhan</span>
                                    <a data-toggle="collapse" href="#spotlight-paper-2-4" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-2-4">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-2-4">
                                        Abstract: Tabular data is prevalent across various machine learning domains. Yet, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, even under few-shot scenarios.
                                    </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="10:00">10:00 AM</th>
                                    <td>Lunch Break</td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="11:30">11:30 AM</th>
                                    <td>
                                        Invited Talk: Immanuel Trummer<br />
                                        <span style="font-style:italic">Next-Generation Data Management with Large Language Models</span>
                                        <br/>
                                        <a data-toggle="collapse" href="#schedule-talk3" aria-cexpanded="false"
                                            aria-controls="schedule-talk3">[Abstract]</a>
                                        <div class="collapse" id="schedule-talk3">
                                            Abstract: The past years have been marked by several breakthrough results in the domain of generative AI, culminating in the rise of tools like ChatGPT, able to solve a variety of language-related tasks without specialized training. In this talk, I discuss several recent research projects at Cornell, exploiting large language models to enhance relational database management systems. These projects cover applications of language models in the database interface, enabling users to specify high-level analysis goals for fully automated end-to-end analysis, as well as applications in the backend, using language models to extract useful information for data profiling and database tuning from text documents.
                                        </div>
                                        <a data-toggle="collapse" href="#speaker-bio-talk3" aria-cexpanded="false"
                                        aria-controls="speaker-bio-talk3">[Speaker Bio]</a>
                                        <div class="collapse" id="speaker-bio-talk3">
                                            Bio: Immanuel Trummer is an assistant professor at Cornell University and a member of the Cornell Database Group. His papers were selected for “Best of VLDB”, “Best of SIGMOD”, for the ACM SIGMOD Research Highlight Award, and for publication in CACM as CACM Research Highlight. His online lecture introducing students to database topics collected over a million views. He received the NSF CAREER Award and multiple Google Faculty Research Awards.
                                        </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="12:00">12:00 PM</th>
                                    <td>Spotlight Talks: Session 3 (Tables + LLMs)</td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="12:00">12:00 PM</span> ~
                                    <span scope="row" data-time="12:07">12:07 PM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=Ld5UCpiT07" target="_blank">Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs </a>
                                    <br/>
                                    <span class="paper-authors">Ananya Singha, José Cambronero, Sumit Gulwani, Vu Le, Chris Parnin</span>
                                    <a data-toggle="collapse" href="#spotlight-paper-3-1" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-3-1">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-3-1">
                                        Abstract: Large language models (LLMs) are increasingly applied for tabular tasks usingin-context learning. The prompt representation for a table may play a role in theLLMs ability to process the table. Inspired by prior work, we generate a collectionof self-supervised structural tasks (e.g. navigate to a cell and row; transpose thetable) and evaluate the performance differences when using 8 formats. In contrastto past work, we introduce 8 noise operations inspired by real-world messy dataand adversarial inputs, and show that such operations can impact LLM performanceacross formats for different structural understanding tasks.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="12:08">12:08 PM</span> ~
                                    <span scope="row" data-time="12:15">12:15 PM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=5sOZNkkKh3" target="_blank">How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings</a>
                                    <br>
                                    <span class="paper-authors">Shuaichen Chang, Eric Fosler-Lussier</span>
                                    <a data-toggle="collapse" href="#spotlight-paper-3-2" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-3-2">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-3-2">
                                        Abstract: Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights into prompt constructions for future text-to-SQL studies.
                                    </div>
                                    </td>
                                </tr>
                                <tr >
                                    <th></th>
                                    <td>
                                    <span scope="row" data-time="12:16">12:16 PM</span> ~
                                    <span scope="row" data-time="12:23">12:23 PM</span>
                                    <a class="paper-title" href="https://openreview.net/forum?id=EocsZtcA7P" target="_blank">IngesTables: Scalable and Efficient Training of LLM-Enabled Tabular Foundation Models</a>
                                    <br>
                                    <span class="paper-authors">Scott Yak, Yihe Dong, Javier Gonzalvo, Sercan Arik</span>
                                    <a data-toggle="collapse" href="#spotlight-paper-3-3" aria-cexpanded="false"
                                        aria-controls="spotlight-paper-3-3">[Abstract]</a>
                                    <div class="collapse" id="spotlight-paper-3-3">
                                        Abstract: There is a massive amount of tabular data that can be taken advantage of via `foundation models' to improve prediction performance for downstream tabular prediction tasks. However, numerous challenges constitute bottlenecks in building tabular foundation models, including learning semantic relevance between tables and features, mismatched schemes, arbitrarily high cardinality for categorical values, and scalability to many tables, rows and features. We propose \texttt{IngesTables}, a novel canonical tabular foundation model building framework, designed to address the aforementioned challenges. \texttt{IngesTables} employs LLMs to encode representations of table/feature semantics and the relationships, that are then modeled via an attention-based tabular architecture. Unlike other LLM-based approaches, \texttt{IngesTables} is much cheaper to train and faster to run inference, because of how LLM-generated embeddings are defined and cached.We show that \texttt{IngesTables} demonstrates significant improvements over commonly-used models like XGBoost on clinical trial datasets in standard supervised learning settings, and is competitive with tabular prediction models that are specialized for clinical trial datasets without incurring LLM-level cost and latency.
                                    </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="12:30">12:30 PM</th>
                                    <td>
                                        Invited Talk: Tao Yu<br />
                                        <span style="font-style:italic">Advancing Natural Language Interfaces to Data with Language Models as Agents
                                        </span>
                                        <br/>
                                        <a data-toggle="collapse" href="#schedule-talk4" aria-cexpanded="false"
                                            aria-controls="schedule-talk4">[Abstract]</a>
                                        <div class="collapse" id="schedule-talk4">
                                            Abstract: Traditional Natural Language Interfaces (NLIs) to data often necessitate users to provide detailed, step-by-step instructions, reflecting an assumption of user familiarity with the underlying data and systems, which can limit accessibility. The emergence of Large Language Models (LLMs) has, however, revolutionized NLIs, enabling them to perform sophisticated reasoning, decision-making, and planning multi-step actions in diverse environments autonomously. In this talk, I will discuss how these language models as agents facilitate a paradigm shift towards moving beyond traditional code generation to more autonomous and user-friendly NLIs, capable of understanding high-level objectives without requiring intricate directives. I will also present our latest work in this direction, including instruction-finetuned retrievers for diverse environment adaptation, the enhancement of LLM capabilities with tool integration, and the development of open, state-of-the-art LLMs and platforms for constructing such language agents. The talk will conclude with an exploration of the current and future research prospects in this rapidly evolving domain.
                                        </div>
                                        <a data-toggle="collapse" href="#speaker-bio-talk4" aria-cexpanded="false"
                                        aria-controls="speaker-bio-talk4">[Speaker Bio]</a>
                                        <div class="collapse" id="speaker-bio-talk4">
                                            Bio: Tao Yu is an Assistant Professor of Computer Science at The University of Hong Kong and a director of the XLANG Lab (as part of the HKU NLP Group). He spent one year in the UW NLP Group working with Noah Smith, Luke Zettlemoyer, and Mari Ostendorf. He completed his Ph.D. in Computer Science from Yale University, advised by Dragomir Radev and master's at Columbia University advised by Owen Rambow and Kathleen McKeown. Tao has received the Google and Amazon faculty research awards (Google Research Scholar Award 2023, Amazon Research Award 2022). His main research interest is in Natural Language Processing. His research aims to build language model agents that transform (“grounding”) language instructions into code or actions executable in real-world environments, including databases, web applications, and the physical world etc,. It lies at the heart of the next generation of natural language interfaces that can interact with and learn from these real-world environments to facilitate human interaction with data analysis, web applications, and robotic instruction through conversation.
                                        </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="13:00">01:00 PM</th>
                                    <td>Coffee Break / Poster Setup</td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="13:20">01:20 PM</th>
                                    <td>Poster: Session 2

                                        <a data-toggle="collapse" href="#poster-2" aria-cexpanded="false"
                                        aria-controls="poster-2">[Details]</a>
                                        <div class="collapse" id="poster-2">
                                        <li>Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks. Soumajyoti Sarkar, Leonard Lausen.</li>

                                        <li>How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. Shuaichen Chang, Eric Fosler-Lussier.</li>

                                        <li>IngesTables: Scalable and Efficient Training of LLM-Enabled Tabular Foundation Models. Scott Yak, Yihe Dong, Javier Gonzalvo, Sercan Arik</li>

                                        <li>In Defense of Zero Imputation for Tabular Deep Learning. Mike Van Ness, Madeleine Udell.</li>

                                        <li>Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning. Felix den Breejen, Sangmin Bae, Stephen Cha, Tae-Young Kim, Seoung Hyun Koh, Se-Young Yun.</li>     

                                        <li>Scaling Experiments in Self-Supervised Cross-Table Representation Learning. Maximilian Schambach, Dominique Paul, Johannes Otterbach.</li>

                                        <li>Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs. Ananya Singha, José Cambronero, Sumit Gulwani, Vu Le, Chris Parnin.</li>

                                        <li>CHORUS: Foundation Models for Unified Data Discovery and Exploration. Moe Kayali, Anton Lykov, Ilias Fountalis, Nikolaos Vasiloglou, Dan Olteanu, Dan Suciu.</li>

                                        <li>Incorporating LLM Priors into Tabular Learners. Max Zhu, Siniša Stanivuk, Andrija Petrovic, Mladen Nikolic, Pietro Lio.</li>

                                        <li>A DB-First approach to query factual information in LLMs. Mohammed Saeed, Nicola De Cao, Paolo Papotti.</li>

                                        <li>Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation. Han-Jia Ye, Qile Zhou, De-Chuan Zhan.</li>

                                        <li>Hopular: Modern Hopfield Networks for Tabular Data. Bernhard Schäfl, Lukas Gruber, Angela Bitto-Nemling, Sepp Hochreiter.</li>

                                        <li>HyperFast: Instant Classification for Tabular Data. David Bonet, Daniel Mas Montserrat, Xavier Giró-i-Nieto, Alexander Ioannidis.</li>

                                        <li>Modeling string entries for tabular data prediction: do we need big large language models? Leo Grinsztajn, Myung Jun Kim, Edouard Oyallon, Gael Varoquaux.</li>

                                        <li>Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains. Kyungeun Lee, Ye Seul Sim, Hyeseung Cho, Suhee Yoon, Sanghyu Yoon, Woohyung Lim.</li>

                                        <li>Self-supervised Representation Learning from Random Data Projectors. Yi Sui, Tongzi Wu, Jesse Cresswell, Ga Wu, George Stein, Xiao Shi Huang, Xiaochen Zhang, Maksims Volkovs.</li>

                                        <li>Unlocking the Transferability of Tokens in Deep Models for Tabular Data. Qile Zhou, Han-Jia Ye, Leye Wang, De-Chuan Zhan.</li>

                                        <li>Augmentation for Context in Financial Numerical Reasoning over Textual and Tabular Data with Large-Scale Language Model. Yechan Hwang, Jinsu Lim, Young-Jun Lee, Ho-Jin Choi.</li>

                                        <li>TabContrast: A Local-Global Level Method for Tabular Contrastive Learning. Hao Liu, Yixin Chen, Bradley Fritz, Christopher King.</li>

                                        <li>GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent. Sascha Marton, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt.</li>
                                        <li>Elephants Never Forget: Testing Language Models for Memorization of Tabular Data. Sebastian Bordt, Harsha Nori, Rich Caruana.</li>

                                        <li>Introducing the Observatory Library for End-to-End Table Embedding Inference. Tianji Cong, Zhenjie Sun, Paul Groth, H.V. Jagadish, Madelon Hulsebos.</li>
                                        </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="14:00">02:00 PM</th>
                                    <td>
                                        Invited Talk: Wenhu Chen<br />
                                        <span style="font-style:italic">Enabling Large Language Models to Reason with Tables</span>
                                        <br/>
                                        <a data-toggle="collapse" href="#schedule-talk5" aria-cexpanded="false"
                                            aria-controls="schedule-talk5">[Abstract]</a>
                                        <div class="collapse" id="schedule-talk5">
                                            Abstract: Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack comprehensive studies examining whether LLMs can truly comprehend such data. In this talk, I will cover different ways to utilize LLMs to interface with tables. One approach is to feed the whole table as a sequence to LLMs for reasoning. In this direction, we will talk about the recent paper GPT4Table to summarize the lessons learned in different table linearization strategies, including table input format, content order, role prompting, and partition marks. The other approach is to use tools like SQL or other language to interface with table for data access without feeding the entire table. LLMs will work as a reasoner to derive the answer based on the interfaced results from the table.
                                        </div>
                                        <a data-toggle="collapse" href="#speaker-bio-talk5" aria-cexpanded="false"
                                        aria-controls="speaker-bio-talk5">[Speaker Bio]</a>
                                        <div class="collapse" id="speaker-bio-talk5">
                                            Bio: Wenhu Chen has been an assistant professor at Computer Science Department in University of Waterloo and Vector Institute since 2022. He obtained Canada CIFAR AI Chair Award in 2022. He also works for Google Deepmind as a part-time research scientist since 2021. Before that, he obtained his PhD from the University of California, Santa Barbara under the supervision of William Wang and Xifeng Yan. His research interest lies in natural language processing, deep learning and multimodal learning. He aims to design models to handle complex reasoning scenarios like math problem-solving, structure knowledge grounding, etc. He is also interested in building more powerful multimodal models to bridge different modalities. He received the Area Chair Award in AACL-IJCNLP 2023, the Best Paper Honorable Mention in WACV 2021, and the UCSB CS Outstanding Dissertation Award in 2021.
                                        </div>
                                    </td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="14:30">02:30 PM</th>
                                    <td>Panel - TBA</td>
                                </tr>
                                <tr>
                                    <th scope="row" data-time="15:15">03:15 PM</th>
                                    <td>Closing Notes</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    <hr />

    <div>
        <div class="row" id="cfp">
            <div class="col-xs-12">
                <h2>Call for Papers</h2>
            </div>
        </div>
    </div>
    <br>
    <div class="row" id="dates">
        <div class="col-xs-12">
            <h3>Important Dates</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <br>
            <table class="table table-striped">
                <tbody>
                    <tr>
                        <td>Submission Open</td>
                        <td>August 10, 2023</td>
                    </tr>
                    <tr>
                        <td>Submission Deadline</td>
                        <td><s><b>October <b>4</b>, 2023</b> (11:59PM AoE)</s></td>
                    </tr>
                    <tr>
                        <td>Notifications</td>
                        <td><s>October 27, 2023 (11:59PM AoE)</s></td>
                    </tr>
                    <tr>
                        <td>Camera-ready</td>
                        <td><s>November 15, 2023 (11:59PM AoE)</s></td>
                    </tr>
                    <tr>
                        <td>Slides for spotlight talks</td>
                        <td><s>November 28, 2023 (11:59PM AoE)</s></td>
                    </tr>
                    <tr>
                        <td >Video pitches for posters</td>
                        <td ><s>November 28, 2023 (11:59PM AoE)</s></td>
                    </tr>
                    <tr>
                        <td>Workshop Date</td>
                        <td><b>December 15, 2023</b></td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <h3>Scope</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">

            <p>We invite submissions on representation and generative learning over tables, related to any of the following topics:
            </p>

            <ul>
                <li><b>Representation Learning over tables  </b> which can be structured as well as semi-structured, and extend to full databases. 
                    Example contributions are new model architectures, data encoding techniques, pre-training,
                    fine-tuning, and prompting strategies, multi-task learning, etc.
                </li>
                <li><b>Generative Learning and LLMs  </b>  for structured data and interfaces to structured data
                    (e.g. queries, analysis).
                </li>
                <li><b>Multimodal learning  </b>
                    where tables are jointly embedded with, for example, natural language, code (e.g. SQL),
                    knowledge bases, visualizations/images.
                </li>
                <li><b>Downstream Applications  </b> of table representations for tasks like data preparation
                    (e.g. data cleaning, validation, integration, cataloging, feature engineering),
                    retrieval (e.g. search, fact-checking/QA, KG construction),
                    analysis (e.g. summarization, visualization, and query recommendation), and (end-to-end) machine learning.
                </li>
                <li><b>Upstream Applications  </b> of table representation models for optimizing table parsers/extraction
                    (from documents, spreadsheets, presentations), storage (e.g. compression, indexing)
                    and query processing e.g. query optimization
                </li>
                <li><b>Production challenges  </b>of table representation models.
                    Work addressing the challenges of maintaining and managing TRL models in fast evolving contexts,
                    e.g. data updating, error correction, and monitoring, and other industry challenges such as privacy, personalization performance, etc.
                </li>
                <li><b>Domain-specific challenges  </b>for learned table models often arise in domains such as enterprise,
                    finance, medical, law. These challenges pertain to table content, table structure, privacy, security
                    limitations, and other factors that necessitate tailored solutions.
                </li>
                <li><b>Benchmarks and analyses  </b>of table representation models, including the utility of language models as base models
                    versus alternatives and robustness regarding large, messy, heterogeneous, or complex tables.
                </li>
                <li><b>Others: </b> Formalization, surveys, datasets, visions, and reflections to structure and guide future
                    research.
                </li>
            </ul>
        </div>
    </div>

    <hr />

    <div class="row" id="guidelines">
        <div class="col-xs-12">
            <h2>Submission Guidelines</h2>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <h3>Submission link</h3>
            Submit your (anonymized) paper through OpenReview at: <b><a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL" target="blank">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL</a></b><br>
            Please be aware that accepted papers are expected to be presented at the workshop in-person.
        </div>

        <div class="col-xs-12">
            <br>
            <h3>Formatting guidelines</h3>
            The workshop accepts regular research papers and industrial papers of the following types:
            <ul>
                <div class="col-xs-12">
                    <li>Short paper: 4 pages + references.</li>
                    <li>Regular paper: 8 pages + references.</li>
                </div>
            </ul>
            <br>
            <br>
            Submissions should be anonymized and follow the NeurIPS style files, but can exclude the checklist.
            Non-anonymous preprints are no problem, and artifacts do not have to be anonymized. Just submitting the paper without author names/affiliations is sufficient.
            Supplementary material, if any, may be added in the appendix.
            The footer of accepted papers should state “Table Representation Learning Workshop at NeurIPS 2023”. We expect authors to adopt an inclusive and diverse writing style.
            The <a href="https://dbdni.github.io/pages/inclusivewriting.html" target="blank">“Diversity and Inclusion in Writing”</a> guide by the DE&I in DB Conferences effort is a good resource.
        </div>
        
        <div class="col-xs-12">
            <br>
            <h3>Review process</h3>
            Papers will receive light reviews in a double-anonymous manner.
            All accepted submissions will be published on the website and made public on OpenReview but the workshop is non-archival (i.e. without proceedings).
        </div>

        <div class="col-xs-12">
            <br>
            <h3>Novelty and conflicts</h3>
            The workshop does not accept submissions that have previously been published at NeurIPS or other machine learning venues.
            However, we do welcome submissions that have been published in, for example, data management or natural language processing venues.
            We rely on OpenReview for handling conflicts, so please ensure that the conflicts in every author's OpenReview profile are complete, in particular, with respect to the organization and program committees.
            <br>
        </div>

        <div class="col-xs-12">
            <br>
            <h3 style="color: red">Camera-ready instructions</h3>
            Camera-ready papers are expected to express the authors and affiliations on the first page, and state "Table Representation Learning Workshop at NeurIPS 2023'' in the footer.
            The camera-ready version may exceed the page limit for acknowledgements or small content changes, but revision is not required (for short papers: please be aware of novelty requirements of archival venues, e.g. SIGMOD, CVPR).
            The camera-ready version should be submitted through OpenReview (submission -> edit -> revision), and will be published on OpenReview and this website. Please make sure that all meta-data is correct as well, as it will be imported to the NeurIPS website.
        </div>

        <div class="col-xs-12">
            <br>
            <h3 style="color: red">Presentation instructions</h3>
            All accepted papers will be presented as poster during one of the poster sessions (TBA). For poster formatting, please refer to the poster instructions on <a href="https://neurips.cc/FAQ/PosterInformation" target="blank">the NeurIPS site</a>, you can print and bring the poster yourself or consider the <a href="https://docstore.fedex.com/hco5670b" target="blank">FedEx offer for NeurIPS</a>.
            Optional: authors of poster submissions are also invited to send a teaser video of approx. 3 minutes (.mp4) to m.hulsebos@uva.nl, which will be hosted on the website and <a href="https://www.youtube.com/channel/UCON8k4OzdrX9iswYIwb14jQ" target="blank"> YouTube channel</a> of the workshop.
            <br>
            Papers selected for spotlight talks are also asked to prepare a talk of 6 minutes (+1 min Q&A), and upload their slides through the "slides" field in OpenReview. Timeslots for the spotlights will be published soon. The recordings of oral talks will be published as well.
        </div>
    </div>

    <hr />

    <!-- Organizers -->
    <div class="row" id="organization">
        <div class="col-xs-12">
            <h2>Organization</h2>
        </div>

    <div class="col-xs-12">
        <h3>Workshop Chairs</h3>
        <br>
        <div class="col-xs-6 col-lg-3">
            <a href="https://www.madelonhulsebos.com/" target="blank">
                <img class="people-pic" src="assets/people/mh.jpg">
            </a>
            <div class="people-name">
                <a href="https://www.madelonhulsebos.com/" target="blank">Madelon Hulsebos</a>
                <h6>UC Berkeley</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://www.microsoft.com/en-us/research/people/hadong" target="blank">
                <img class="people-pic" src="assets/people/hd.jpg">
            </a>
            <div class="people-name">
                <a href="https://www.microsoft.com/en-us/research/people/hadong" target="blank">Haoyu Dong</a>
                <h6>Microsoft</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://bojan.ninja" target="blank">
                <img class="people-pic" src="assets/people/bk.jpg">
            </a>
            <div class="people-name">
                <a href="https://bojan.ninja" target="blank">Bojan Karlas</a>
                <h6>Harvard</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://cs.stanford.edu/people/lorr1" target="blank">
                <img class="people-pic" src="assets/people/lo.jpg">
            </a>
            <div class="people-name">
                <a href="https://cs.stanford.edu/people/lorr1" target="blank">Laurel Orr</a>
                <h6>Numbers Station AI</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://research.google/people/PengchengYin/" target="blank">
                <img class="people-pic" src="assets/people/py.jpg">
            </a>
            <div class="people-name">
                <a href="https://research.google/people/PengchengYin/" target="blank">Pengcheng Yin</a>
                <h6>Google DeepMind</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://gael-varoquaux.info/" target="blank">
                <img class="people-pic" src="assets/people/gv.jpg">
            </a>
            <div class="people-name">
                <a href="https://gael-varoquaux.info/" target="blank">Gaël Varoquaux</a>
                <h6>INRIA</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://siviltaram.github.io/" target="blank">
                <img class="people-pic" src="assets/people/ql.png">
            </a>
            <div class="people-name">
                <a href="https://siviltaram.github.io/" target="blank">Qian Liu</a>
                <h6>Sea AI Lab</h6>
                <br>
            </div>
        </div>
    </div>

    <div class="col-xs-12">
        <h3>Program Committee</h3>
        <a data-toggle="collapse" href="#pc" aria-cexpanded="false"
        aria-controls="pc">Unfold for full committee</a>
        <div class="collapse" id="pc">
            Vadim Borisov,	University of Tuebingen<br>
            Paul Groth,	University of Amsterdam<br>
            Wensheng Dou,	Institute of Software Chinese Academy of Sciences<br>
            Hiroshi Iida,	The University of Tokyo<br>
            Sharad Chitlangia,	Amazon<br>
            Jaehyun Nam,	KAIST<br>
            Jinyang Li,	The University of Hong Kong<br>
            Gerardo Vitagliano,	Hasso Plattner Institute<br>
            Rajat Agarwal,	Amazon<br>
            Micah Goldblum,	New York University<br>
            Yury Gorishniy,	Yandex Research<br>
            Roman Levin,	Amazon<br>
            Bhavesh Neekhra,	Ashoka University<br>
            Sebastian Schelter,	University of Amsterdam<br>
            Qingping Yang,	University of Chinese Academy of Sciences<br>
            Matteo Interlandi,	Microsoft<br>
            Tianji Cong,	University of Michigan<br>
            Xiang Deng,	Google<br>
            Beliz Gunel,	Google<br>
            Qian Liu,	Sea AI Lab<br>
            Shuaichen Chang,	Ohio State University<br>
            Zhoujun Cheng,	Shanghai Jiaotong University<br>
            Roee Shraga,	Worcester Polytechnic Institute<br>
            Yi Zhang,	AWS AI Labs<br>
            Xi Rao,	ETH Zurich<br>
            Liane Vogel, Technical University of Darmstadt<br>
            Aneta Koleva, University of Munich / Siemens<br>
            Ivan Rubachev, HSE University / Yandex<br>
            Meghana Moorthy Bhat, Salesforce Research<br>
            José Cambronero, Microsoft<br>
            Till Döhmen, MotherDuck / University of Amsterdam<br>
            Noah Hollman, Charité Berlin / University of Freiburg<br>
            Julian Martin Eisenschlos, Google<br>
            Paolo Papotti, Eurecom<br>
            Zhiruo Wang, Carnegie Mellon University<br>
            Mukul Singh, Microsoft<br>
            Zezhou Huang, Columbia University<br>
            Carsten Binnig, TU Darmstadt<br>
            Linyong Nan, Yale<br>
            Shuo Zhang, Bloomberg<br>
            Alejandro Sierra Múnera, Hasso Plattner Institute<br>
            Qian Liu, Sea AI Labs<br>
            Anirudh Khatry, Microsoft<br>
            Haoyu Dong, Microsoft<br><br>
        </div>
    </div>


    <!-- <div class="col-xs-12">
        <h3>Review for TRL?</h3>
        We are compiling a PC for light reviewing of submissions to TRL 2023 (at most 3 reviews per reviewer).<br>
        We invite researchers with expertise relevant to TRL to express their interest in reviewing through the below form.<br>
        Your time and expertise is much appreciated!
        <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSdjlbY-590PzDNBtOlUlCzxD_Uhbkmq389pRieW7L5B7F31_g/viewform?embedded=true" width="680" height="320" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
        <br>
    </div> -->

    </div>

    <hr />

    <div class="row" id="accepted-papers">
        <div class="col-xs-12">
            <h2>Accepted Papers</h2>
            <!-- <p>Note: 2 additional papers were accepted but are not listed here because of an anonymity period.</p> -->
        </div>
    </div>

    <br/>

    <div>
        <h3>2023</h3>

        <br/>

        <h4>Oral</h4>
        
        <ul class="paper-list">
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=zxaoBcdACd" target="_blank">MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering</a>
                <br>
                <span class="paper-authors">Vaishali Pal, Andrew Yates, Evangelos Kanoulas, Maarten Rijke
                <br>
            </li>

            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=jSebr3OJA2" target="_blank">GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data</a>
                <br>
                <span class="paper-authors">Andrei Margeloiu, Nikola Simidjievski, Pietro Lió, Mateja Jamnik
                <br>
            </li>

                        
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=gs6yfSvwue" target="_blank">High-Performance Transformers for Table Structure Recognition Need Early Convolutions</a>
                <br>
                <span class="paper-authors">ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari Balasubramaniyan, Duen Horng Chau
                <br>
            </li>

            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=dkeZPuFmIz" target="_blank">Self-supervised Representation Learning from Random Data Projectors</a>
                <br>
                <span class="paper-authors">Yi Sui, Tongzi Wu, Jesse Cresswell, Ga Wu, George Stein, Xiao Shi Huang, Xiaochen Zhang, Maksims Volkovs
                <br>
            </li>

            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=VRBhaU8IDz" target="_blank">HyperFast: Instant Classification for Tabular Data</a>
                <br>
                <span class="paper-authors">David Bonet, Daniel Mas Montserrat, Xavier Giró-i-Nieto, Alexander Ioannidis
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=UkP1BSm2tt" target="_blank">Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation</a>
                <br>
                <span class="paper-authors">Han-Jia Ye, Qile Zhou, De-Chuan Zhan
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=Ld5UCpiT07" target="_blank">Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs</a>
                <br>
                <span class="paper-authors">Ananya Singha, José Cambronero, Sumit Gulwani, Vu Le, Chris Parnin
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=FflKTuIRTD" target="_blank">Data Ambiguity Strikes Back: How Documentation Improves GPT&#x27;s Text-to-SQL</a>
                <br>
                <span class="paper-authors">Zachary Huang, Pavan Kalyan Damalapati, Eugene Wu
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=EocsZtcA7P" target="_blank">IngesTables: Scalable and Efficient Training of LLM-Enabled Tabular Foundation Models</a>
                <br>
                <span class="paper-authors">Scott Yak, Yihe Dong, Javier Gonzalvo, Sercan Arik
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=6Kb3pE9nWQ" target="_blank">Pool-Search-Demonstrate: Improving Data-wrangling LLMs via better in-context examples</a>
                <br>
                <span class="paper-authors">Joon Suk Huh, Changho Shin, Elina Choi
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=5sOZNkkKh3" target="_blank">How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings</a>
                <br>
                <span class="paper-authors">Shuaichen Chang, Eric Fosler-Lussier
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=4MkkNsAEmO" target="_blank">TabPFGen – Tabular Data Generation with TabPFN</a>
                <br>
                <span class="paper-authors">Jeremy (Junwei) Ma, Apoorv Dankar, George Stein, Guangwei Yu, Anthony Caterini
                <br>
            </li>
        </ul>
        
        <h4>Poster</h4>

        <br/>
        <ul class="paper-list">            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=zWzgulKtvw" target="_blank">Generating Data Augmentation Queries Using Large Language Models</a>
                <br>
                <span class="paper-authors">Christopher Buss, Jasmin Mousavi, Mikhail Tokarev, Arash Termehchy, David Maier, Stefan Lee
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=uzVCoZSfly" target="_blank">ReConTab: Regularized Contrastive Representation Learning for Tabular Data</a>
                <br>
                <span class="paper-authors">Suiyao Chen, Jing Wu, NAIRA HOVAKIMYAN, Handong Yao
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=u2OVQ2Xvq1" target="_blank">Unlocking the Transferability of Tokens in Deep Models for Tabular Data</a>
                <br>
                <span class="paper-authors">Qile Zhou, Han-Jia Ye, Leye Wang, De-Chuan Zhan
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=s0rM4hWBUq" target="_blank">Augmentation for Context in Financial Numerical Reasoning over Textual and Tabular Data with Large-Scale Language Model</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://youtu.be/MlOnUW1AQms?si=f8Ifitdq3vQmz95Q" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
                <span class="paper-authors">Yechan Hwang, Jinsu Lim, Young-Jun Lee, Ho-Jin Choi
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=rSuRu22bbN" target="_blank">TabContrast: A Local-Global Level Method for Tabular Contrastive Learning</a>
                <br>
                <span class="paper-authors">Hao Liu, Yixin Chen, Bradley A Fritz, Christopher King
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=pPAK4FIopM" target="_blank">Explaining Explainers: Necessity and Sufficiency in Tabular Data</a>
                <br>
                <span class="paper-authors">Prithwijit Chowdhury, Mohit Prabhushankar, Ghassan AlRegib
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=lsn7ehxAdt" target="_blank">Beyond Individual Input for Deep Anomaly Detection on Tabular Data</a>
                <br>
                <span class="paper-authors">Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Liên DOAN
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=lWBMNF7D8F" target="_blank">GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent</a>
                <br>
                <span class="paper-authors">Sascha Marton, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=l1u7jA60wT" target="_blank">Elephants Never Forget: Testing Language Models for Memorization of Tabular Data</a>
                <br>
                <span class="paper-authors">Sebastian Bordt, Harsha Nori, Rich Caruana
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=kzR5Cj5blw" target="_blank">InterpreTabNet: Enhancing Interpretability of Tabular Data Using Deep Generative Models and Large Language Models</a>
                <br>
                <span class="paper-authors">Jacob Yoke Hong Si, Rahul Krishnan, Michael Cooper, Wendy Yusi Cheng
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=k5Mnd4pO7X" target="_blank">On Incorporating new Variables during Evaluation</a>
                <br>
                <span class="paper-authors">Harsimran Bhasin, Soumyadeep Ghosh
                <br>
            </li>
            
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=ejfKPO9aw0" target="_blank">Unnormalized Density Estimation with Root Sobolev Norm Regularization</a>
                <br>
                <span class="paper-authors">Mark Kozdoba, Binyamin Perets, Shie Mannor
                <br>
            </li>
            
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=dQLDxIPsU4" target="_blank">Tree-Regularized Tabular Embeddings</a>
                <br>
                <span class="paper-authors">Xuan Li, Yun Wang, Bo Li
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=btK3lk5puP" target="_blank">Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains</a>
                <br>
                <span class="paper-authors">Kyungeun Lee, Ye Seul Sim, Hyeseung Cho, Suhee Yoon, Sanghyu Yoon, Woohyung Lim
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=b4GEmjsHAB" target="_blank">A Deep Learning Blueprint for Relational Databases</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://youtu.be/1QUQogj_xmI?si=gOFsXhhuG1EYGwWm" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
                <span class="paper-authors">Lukáš Zahradník, Jan Neumann, Gustav Šír
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=b0OhN0ii36" target="_blank">Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks</a>
                <br>
                <span class="paper-authors">Benjamin Feuer, Niv Cohen, Chinmay Hegde
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=WXNmnmpRBJ" target="_blank">Modeling string entries for tabular data prediction: do we need big large language models?</a>
                <br>
                <span class="paper-authors">Leo Grinsztajn, Myung Jun Kim, Edouard Oyallon, Gael Varoquaux
                <br>
            </li>

            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=V4Pa9B8zRk" target="_blank">Hopular: Modern Hopfield Networks for Tabular Data</a>
                <br>
                <span class="paper-authors">Bernhard Schäfl, Lukas Gruber, Angela Bitto, Sepp Hochreiter
                <br>
            </li>            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=TSkiaPP1sq" target="_blank">NeuroDB: Efficient, Privacy-Preserving and Robust Query Answering with Neural Networks</a>
                <br>
                <span class="paper-authors">Sepanta Zeighami, Cyrus Shahabi
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=R8VFPAfOcN" target="_blank">A DB-First approach to query factual information in LLMs</a>
                <br>
                <span class="paper-authors">Mohammed SAEED, Nicola De Cao, Paolo Papotti
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=PxEY7pBb6F" target="_blank">A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning</a>
                <br>
                <span class="paper-authors">Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping, C. Bayan Bruss, Andrew Wilson, Tom Goldstein, Micah Goldblum
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=OFV0uNeZ7R" target="_blank">Incorporating LLM Priors into Tabular Learners</a>
                <br>
                <span class="paper-authors">Max Zhu, Siniša Stanivuk, Andrija Petrovic, Mladen Nikolic, Pietro Lió
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=MzSNAO2ZlW" target="_blank">CHORUS: Foundation Models for Unified Data Discovery and Exploration</a>
                <br>
                <span class="paper-authors">Moe Kayali, Anton Lykov, Ilias Fountalis, Nikolaos Vasiloglou, Dan Olteanu, Dan Suciu
                <br>
            </li>
                        
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=JIrTIMI5Yd" target="_blank">Introducing the Observatory Library for End-to-End Table Embedding Inference</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://youtu.be/H3ww-MXInrk?si=dlFCspTFifnr8GKI" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
                <span class="paper-authors">Tianji Cong, Zhenjie Sun, Paul Groth, H. V. Jagadish, Madelon Hulsebos
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=IbiiNw4oRj" target="_blank">Scaling Experiments in Self-Supervised Cross-Table Representation Learning</a>
                <br>
                <span class="paper-authors">Maximilian Schambach, Dominique Paul, Johannes Otterbach
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=HtdZSf1ObU" target="_blank">Benchmarking Tabular Representation Models in Transfer Learning Settings</a>
                <br>
                <span class="paper-authors">Qixuan Jin, Talip Ucar
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=HK3MUPgFg4" target="_blank">Exploring the Retrieval Mechanism for Tabular Deep Learning</a>
                <br>
                <span class="paper-authors">Felix den Breejen, Sangmin Bae, Stephen Cha, Tae-Young Kim, Seoung Hyun Koh, Se-Young Yun
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=H0gENXL7F2" target="_blank">In Defense of Zero Imputation for Tabular Deep Learning</a>
                <br>
                <span class="paper-authors">John Van Ness, Madeleine Udell
                <br>
            </li>            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=3gBqMkELhZ" target="_blank">Multitask-Guided Self-Supervised Tabular Learning for Patient-Specific Survival Prediction</a>
                <br>
                <span class="paper-authors">You Wu, Omid Bazgir, Yongju Lee, Tommaso Biancalani, James Lu, Ehsan Hajiramezanali
                <br>
            </li>
            
            <li>
                <a class="paper-title" href="https://openreview.net/forum?id=3L2u0unIHd" target="_blank">Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks</a>
                <br>
                <span class="paper-authors">Soumajyoti Sarkar, Leonard Lausen
                <br>
            </li>
            
        </ul>

        <br/>
        <!-- <b style="color: red">An early list of accepted papers is <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL">here</a></b> -->
    </div>

    <br/>
    <br/>

    <div>
        <h3>2022</h3>

        <br/>

        <h4>Oral</h4>

        <br/>

        <ul class="paper-list">
        <li>
            <a class="paper-title" href="../../assets/papers/analysis_of_the_attention_in_t.pdf" target="_blank">Analysis of the Attention in
                Tabular
                Language Models</a> &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993349" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
            <br>
            <span class="paper-authors">Aneta Koleva, Martin Ringsquandl, Volker Tresp</span>
            <br>
        </li>
        </li>

        <li> <a class="paper-title" href="assets/papers/transfer_learning_with_deep_ta.pdf" target="_blank">Transfer Learning with Deep
                Tabular
                Models</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993348" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
            <br>
            <span class="paper-authors">Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C. Bayan
                Bruss,
                Tom
                Goldstein, Andrew Gordon Wilson, Micah Goldblum
            </span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stable_table_generation_framew.pdf" target="_blank">STable: Table Generation
                Framework
                for
                Encoder-Decoder Models</a>
                    &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993352" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>

                    <br>
            <span class="paper-authors">Michał Pietruszka, Michał Turski, Łukasz Borchmann, Tomasz Dwojak, Gabriela
                Pałka,
                Karolina Szyndler, Dawid Jurkiewicz, Łukasz Garncarek</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/tabpfn_a_transformer_that_solv.pdf" target="_blank">TabPFN: A Transformer That
                Solves
                Small
                Tabular Classification Problems in a Second</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993350" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">Noah Hollmann, Samuel Müller, Katharina Eggensperger, Frank Hutter</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/towards_parameter_efficient_au.pdf" target="_blank">Towards Parameter-Efficient
                Automation
                of Data Wrangling Tasks with Prefix-Tuning</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993351" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">David Vos, Till Döhmen, Sebastian Schelter</span>
        </li>

        <li> <a class="paper-title" href="https://openreview.net/forum?id=7q_-aEdnGZw" target="_blank">RegCLR: A Self-Supervised Framework
                for
                Tabular Representation Learning in the Wild</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38996604" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">Weiyao Wang, Byung-Hak Kim, Varun Ganapathi</span>
        </li>
        </ul>

        <br/>

        <h4>Poster</h4>
        
        <br/>

        <ul class="paper-list">
        <li> <a class="paper-title" href="assets/papers/saint_improved_neural_networks.pdf" target="_blank">SAINT: Improved Neural Networks
                for
                Tabular Data via Row Attention and Contrastive Pre-Training</a><br>
            <span class="paper-authors">Gowthami Somepalli, Avi Schwarzschild, Micah Goldblum, C. Bayan Bruss, Tom
                Goldstein</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/generic_entity_resolution_mode.pdf" target="_blank">Generic Entity Resolution
                Models</a><br>
            <span class="paper-authors">Jiawei Tang, Yifei Zuo, Lei Cao, Samuel Madden</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/towards_foundation_models_for_.pdf" target="_blank">Towards Foundation Models for
                Relational
                Databases</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=GyeGQGmTv30" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                <br>
            <span class="paper-authors">Liane Vogel, Benjamin Hilprecht, Carsten Binnig</span>
        </li>


        <li> <a class="paper-title" href="assets/papers/diffusion_models_for_missing_v.pdf" target="_blank">Diffusion models for missing
                value
                imputation in tabular data</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=URlh7KJfXzM" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                <br>
            <span class="paper-authors">Shuhan Zheng, Nontawat Charoenphakdee</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stab_self_supervised_learning_.pdf" target="_blank">STab: Self-supervised Learning
                for
                Tabular Data</a><br>
            <span class="paper-authors">Ehsan Hajiramezanali, Max W Shen, Gabriele Scalia, Nathaniel Lee Diamant</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/caspr_customer_activity_sequen.pdf" target="_blank">CASPR: Customer Activity
                Sequence
                based
                Prediction and Representation</a><br>
            <span class="paper-authors">Damian Konrad Kowalczyk, Pin-Jung Chen, Sahil Bhatnagar</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/conditional_contrastive_networ.pdf" target="_blank">Conditional Contrastive
                Networks</a><br>
            <span class="paper-authors">Emily Mu, John Guttag</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/self_supervised_representation.pdf" target="_blank">Self-supervised Representation Learning Across Sequential and Tabular Features Using Transformers</a><br>
            <span class="paper-authors">Rajat Agarwal, Anand Muralidhar, Agniva Som, Hemant Kowshik</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/the_need_for_tabular_represent.pdf" target="_blank">The Need for Tabular
                Representation
                Learning: An Industry Perspective</a><br>
            <span class="paper-authors">Joyce Cahoon, Alexandra Savelieva, Andreas C Mueller, Avrilia Floratou, Carlo
                Curino,
                Hiren Patel, Jordan Henkel, Markus Weimer, Roman Batoukov, Shaleen Deep, Venkatesh Emani, Richard
                Wydrowski,
                Nellie Gustafsson</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stunt_few_shot_tabular_learnin.pdf" target="_blank">STUNT: Few-shot Tabular Learning
                with
                <span class="paper-authors">Self-generated Tasks from Unlabeled Tables</a><br>
            Jaehyun Nam, Jihoon Tack, Kyungmin Lee, Hankook Lee, Jinwoo Shin<</li>

        <li> <a class="paper-title" href="assets/papers/tabular_data_generation_can_we.pdf" target="_blank">Tabular Data Generation: Can We
                Fool
                XGBoost?</a>
            <br>
            <span class="paper-authors">EL Hacen Zein, Tanguy Urvoy</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/sima_federating_data_silos_usi.pdf" target="_blank">SiMa: Federating Data Silos using
                GNNs</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=y4ZOobI1v2w" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                    <br>
            <span class="paper-authors">Christos Koutras, Rihan Hai, Kyriakos Psarakis, Marios Fragkoulis, Asterios
                Katsifodimos

        <li> <a class="paper-title" href="assets/papers/self_supervised_pre_training_f.pdf" target="_blank">Self Supervised Pre-training for
                Large Scale Tabular Data</a><br>
            <span class="paper-authors">Sharad Chitlangia, Anand Muralidhar, Rajat Agarwal</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/rotar_efficient_row_based_tabl.pdf" target="_blank">RoTaR: Efficient Row-Based Table
                Representation Learning via Teacher-Student Training</a><br>
            <span class="paper-authors">Zui Chen, Lei Cao, Samuel Madden</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/mapqa_a_dataset_for_question_a.pdf" target="_blank">MapQA: A Dataset for Question
                Answering on Choropleth Maps</a><br>
            <span class="paper-authors">Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, Ningchuan
                Xiao</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/met_masked_encoding_for_tabula.pdf" target="_blank">MET: Masked Encoding for Tabular
                Data</a><br>
            <span class="paper-authors">Kushal Alpesh Majmundar, Sachin Goyal, Praneeth Netrapalli, Prateek Jain</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/active_learning_with_tabular_l.pdf" target="_blank">Active Learning with Table
                Language
                Models</a><br>
            <span class="paper-authors">Martin Ringsquandl, Aneta Koleva</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/structural_embedding_of_data_f.pdf" target="_blank">Structural Embedding of Data
                Files
                with MAGRITTE</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=_seBQIBzFoI" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                    <br>
            <span class="paper-authors">Gerardo Vitagliano, Mazhar Hameed, Felix Naumann</span>
        </li>
        </ul>

    </div>
